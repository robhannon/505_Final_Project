{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# GPT2 Finetuning"
      ],
      "metadata": {
        "id": "gfEyO5JvhurQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofRtzmiZha1K",
        "outputId": "b25d6851-25b3-42e5-a52a-53176c574d95"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#final_0_1 = pd.read_csv('./QAData0_1.csv')\n",
        "#final_0_2 = pd.read_csv('./QAData0_2.csv')\n",
        "#final = pd.concat([final_0_1, final_0_2],ignore_index=True)\n",
        "import pandas as pd\n",
        "\n",
        "df1 = pd.read_csv('./QAData1.csv')\n",
        "# df2 = pd.read_csv('./QAData2.csv')\n",
        "# df3 = pd.read_csv('./QAData3.csv')\n",
        "# df4 = pd.read_csv('./QAData4.csv')\n",
        "# df5 = pd.read_csv('./QAData5.csv')\n",
        "# df6 = pd.read_csv('./QAData6.csv')\n",
        "# df7 = pd.read_csv('./QAData7.csv')\n",
        "\n",
        "final = pd.concat([df1])\n",
        "\n",
        "all_questions = final['questions'].tolist()\n",
        "all_answers = final['answers'].tolist()"
      ],
      "metadata": {
        "id": "Kt43ZS7pZRKl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(all_questions))\n",
        "print(len(all_answers))\n",
        "for i in range(5):\n",
        "  print(\"Q:\", all_questions[i])\n",
        "  print(\"A:\", all_answers[i])\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqN-5TPu_dQG",
        "outputId": "553604b7-dfe5-415b-ca53-da756a243e10"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "39801\n",
            "39801\n",
            "Q:  You're back here in this place where you've won before.  Indiscernible  wind.\n",
            "A:  I love the golf course but the fans are what makes it awesome.  To be able to come back and play and enjoy where I won at but they are so supportive here and it's fun to be able to come back and be a part of it.\n",
            "\n",
            "Q:  I caught a glimpse of you looking at the wall of achievements.  I can't imagine what that must feel like looking at that.\n",
            "A:  Yeah it's an honor.  A lot of names up there that I would've loved to play golf with back in the day and ones I have played with on there.\n",
            "\n",
            "Q:  In terms of making your comeback now can you give us an update on how you're feeling how you're doing and what you're doing to get back to your peak level?\n",
            "A:  I'm just taking it one day at a time really.  It's been a struggle.  I'm not hitting it as well as I used to.  Driving it kind of crooked.  I just can't quite get the motion yet with my shoulder and everything it's going through.\n",
            "\n",
            "Q:  How tough is it to fight through that when you're trying to find the consistency you had for so many years right?\n",
            "A:  Yeah it's pretty tough.  It's pretty tough to find.  I don't know man.  I reckon just go out there and just keep grinding away.  I don't want to give up on my Godgiven talent.  That's the thing about it.  He's looked after me and took care of me and gave me another chance to be able to come out here and do this and have fun.\n",
            "\n",
            "Q:  Does it give you a different perspective having to fight through it this way?\n",
            "A:  Oh yeah.  It gives me a lot bigger perspective especially my kids and stuff like that.  Want to spend more time with them and try to be with them as much as you can.  You don't ever know.  It's life.  You can get in a car right now and just drive down the road and something bad can happen.  You just don't never know.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat QA with GPT-2"
      ],
      "metadata": {
        "id": "zI5rnbFwrp4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "XL_BJ_frepzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
      ],
      "metadata": {
        "id": "TzkfkBOoer1K"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.add_special_tokens({\"pad_token\": \"<pad>\",\n",
        "                                \"bos_token\": \"<startofstring>\",\n",
        "                                \"eos_token\": \"<endofstring>\"})\n",
        "tokenizer.add_tokens([\"<ans>:\"])\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "5g9iRPWSey4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class QAData(Dataset):\n",
        "    def __init__(self, tokenizer, q, a):\n",
        "        self.X = []\n",
        "        for i in range(len(q)):\n",
        "          self.X.append(\"<startofstring> \"+ str(q[i]) +\" <ans>: \"+str(a[i])+\" <endofstring>\")\n",
        "\n",
        "        print(self.X[0])\n",
        "\n",
        "        self.X_encoded = tokenizer(self.X, max_length=128, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "        self.input_ids = self.X_encoded['input_ids']\n",
        "        self.attention_mask = self.X_encoded['attention_mask']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (self.input_ids[idx], self.attention_mask[idx])"
      ],
      "metadata": {
        "id": "vDKMyQ-3jNWO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = QAData(tokenizer, all_questions, all_answers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ijz1IlBlpDs",
        "outputId": "4500ef22-10b5-4be5-b85f-5f8faf4c81d5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<startofstring>  You're back here in this place where you've won before.  Indiscernible  wind. <ans>:  I love the golf course but the fans are what makes it awesome.  To be able to come back and play and enjoy where I won at but they are so supportive here and it's fun to be able to come back and be a part of it. <endofstring>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "QAData =  DataLoader(ds, batch_size=16)"
      ],
      "metadata": {
        "id": "_-jOO_sFl3uf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the model"
      ],
      "metadata": {
        "id": "1dZWefMH1TkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Device configuration\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Initialize the model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Add your special tokens and resize token embeddings\n",
        "# (assuming you have custom tokens like in your previous setup)\n",
        "tokenizer.add_special_tokens({\"pad_token\": \"<pad>\",\n",
        "                              \"bos_token\": \"<startofstring>\",\n",
        "                              \"eos_token\": \"<endofstring>\",\n",
        "                              \"additional_special_tokens\": [\"<ans>:\"]})\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Define the optimizer\n",
        "optim = Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Define the Dataset and DataLoader (ensure QAData class and dataset are defined)\n",
        "# Assuming all_questions and all_answers are defined\n",
        "# ds = QAData(tokenizer, all_questions, all_answers)\n",
        "QAData = DataLoader(ds, batch_size=16)\n",
        "\n",
        "# Define the training loop\n",
        "def train(QADataLoader, model, optim):\n",
        "    model.train()\n",
        "    epochs = 1\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        for X, a in QADataLoader:\n",
        "            X = X.to(device)\n",
        "            a = a.to(device)\n",
        "            optim.zero_grad()\n",
        "            loss = model(X, attention_mask=a, labels=X).loss\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "        # Save the model after each epoch\n",
        "        torch.save(model.state_dict(), f\"model_state_epoch_{epoch}.pt\")\n",
        "\n",
        "    # Save the tokenizer after training is complete\n",
        "    tokenizer.save_pretrained('trained_tokenizer')\n",
        "\n",
        "# Train the model\n",
        "print(\"training .... \")\n",
        "train(QAData, model, optim)\n"
      ],
      "metadata": {
        "id": "47Li2_i0mo_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading, tuning, and validating the model"
      ],
      "metadata": {
        "id": "VdkkNtNk1gAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load the model\n",
        "def load_model(model_path, tokenizer_path):\n",
        "    # Load the trained tokenizer\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)\n",
        "    # Initialize the model\n",
        "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\",\n",
        "                                             pad_token_id=tokenizer.eos_token_id)\n",
        "    # Update the token embeddings\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "    # Load the saved weights\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.to(device)\n",
        "    return model, tokenizer\n",
        "\n",
        "def infer(prompt, model, tokenizer, tuned):\n",
        "    model.eval()\n",
        "    if tuned:\n",
        "      prompt = f\"<startofstring> {prompt} <ans>: \"\n",
        "    encoded_input = tokenizer(prompt, return_tensors='pt')\n",
        "    input_ids = encoded_input['input_ids'].to(device)\n",
        "    attention_mask = encoded_input['attention_mask'].to(device)\n",
        "\n",
        "    max_length = 150\n",
        "    temperature = 1.2\n",
        "    top_k = 50\n",
        "    top_p = 0.9\n",
        "    repetition_penalty = 1.7\n",
        "\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_length=max_length,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "        repetition_penalty=repetition_penalty\n",
        "    )\n",
        "\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model, tokenizer = load_model(\"model_state_epoch_0.pt\", 'trained_tokenizer')\n",
        "\n",
        "# Define the sports broadcaster role prompt\n",
        "#role_prompt = \"I am a sports broadcaster that commentates on sorts games and provides exciting insights. Here's my commentary: \"\n",
        "\n",
        "# Run the chatbot with custom prompting\n",
        "print(\"Sports QA Bot: \")\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() == \"quit\":\n",
        "        break\n",
        "    #response = infer(user_input, model, tokenizer, role_prompt)\n",
        "    response = infer(user_input, model, tokenizer, True)\n",
        "\n",
        "    print(f\"Sports Bot: {response}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6apYxmwmXz-",
        "outputId": "2f3e7db2-63bf-48f5-94b2-192c4872fe29"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sports QA Bot: \n",
            "You: What?\n",
            "Sports Bot:  What?   I think it's a little bit of an adjustment. It was just the first time we were playing in Zhuhai and then you have to play against her so she has some good shots but also like last year when we played here with her at home that is what we did well because they're really tough opponents for us especially on grass courts which are very difficult conditions. So yeah this week we've been practicing more than once every day since practice even though there wasn't much break between practices or matches. But now as soon after our match we had two days off we practiced again. We didn't get any points from them today till next week. And I'm happy about my performance today. \n",
            "You: quit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tuned_model, fine_tuned_tokenizer = load_model(\"model_state_epoch_0.pt\", 'trained_tokenizer')\n",
        "# Load the default pre-trained model and tokenizer\n",
        "default_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "default_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "default_model.to(device)"
      ],
      "metadata": {
        "id": "6VHgHDwVLcFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparing the general model to the fine-tuned model"
      ],
      "metadata": {
        "id": "LFY0p1cn1rcG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What?\"\n",
        "\n",
        "# Generate text with the fine-tuned model\n",
        "print(\"Text from Fine-tuned Model:\")\n",
        "print(infer(prompt, fine_tuned_model, fine_tuned_tokenizer, True))\n",
        "\n",
        "# Generate text with the default pre-trained model\n",
        "print(\"\\nText from Default Pre-trained Model:\")\n",
        "print(infer(prompt, default_model, default_tokenizer, False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdQYU0ADMB_s",
        "outputId": "45fe4942-6fb2-4203-e3c2-87353707fa57"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text from Fine-tuned Model:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " What?   I think it's a little bit of an adjustment. It was just the first time we were playing in Zhuhai and then you have to play against her so she has some good shots but also like last year when we played here with her at home that is what we did well because they're really tough opponents for us especially on grass courts which are very difficult conditions. So yeah this week we've been practicing more than once every day since practice even though there wasn't much break between practices or matches. But now as soon after our match we had two days off we practiced again. We didn't get any points from them today till next week. And I'm happy about my performance today. \n",
            "\n",
            "Text from Default Pre-trained Model:\n",
            "What?\n",
            "The first thing I noticed was that the \"I'm a fan of your work\" message on my profile page. It's not like you're saying, 'Hey guys! Let me know if this is something we want to do.' You just say it and then there are some people who will respond with an answer.\" And so when someone says they don't have any interest in working for us or anything else because their job description doesn't match our company name (which isn', well…we've got no idea what those words mean), all sorts go out about how much money these companies make from advertising revenue alone – which means more than $1 billion per year at least; but also less-than half as many dollars spent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "xaW__Nj7Vtme"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests"
      ],
      "metadata": {
        "id": "QF3D3EvIVeZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using GPT and the Fine-Tuned model for realistic text generation"
      ],
      "metadata": {
        "id": "7A4PRWQY11DX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "def query_openai(prompt, api_key):\n",
        "    url = \"https://api.openai.com/v1/chat/completions\"\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": f\"Bearer {api_key}\"\n",
        "    }\n",
        "    data = {\n",
        "        \"model\": \"gpt-4\",\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "        \"temperature\": 0.1\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
        "    return response.json()\n",
        "\n",
        "# Replace 'your_api_key_here' with your actual OpenAI API key\n",
        "api_key = \"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\"\n",
        "\n",
        "prompt = (\"You are a sports caster. Make up a fake play-by-play of a game. \"\n",
        "          \"Describe the actions, the atmosphere, the crowd's reactions. \"\n",
        "          \"Now, transition into an interview with a player. Say 'BEGIN INTERVIEW'. \"\n",
        "          \"Generate two random interview questions, each enclosed with 'START QUESTION' and 'END QUESTION' tags. DO NOT include responses to these questions only generate the questions.\"\n",
        "          \"Conclude the interview with 'END INTERVIEW'.\")\n",
        "\n",
        "\n",
        "response = query_openai(prompt, api_key)\n",
        "generated_response = response['choices'][0]['message']['content']\n",
        "\n",
        "# Function to extract questions from the response\n",
        "def extract_tagged_questions(text):\n",
        "    interview_start = text.find(\"BEGIN INTERVIEW\")\n",
        "    interview_end = text.find(\"END INTERVIEW\", interview_start)\n",
        "    interview_text = text[interview_start:interview_end] if interview_start != -1 and interview_end != -1 else \"\"\n",
        "\n",
        "    questions = []\n",
        "    question_start = interview_text.find(\"START QUESTION\")\n",
        "    while question_start != -1:\n",
        "        question_end = interview_text.find(\"END QUESTION\", question_start)\n",
        "        if question_end != -1:\n",
        "            question_text = interview_text[question_start + len(\"START QUESTION\"):question_end].strip()\n",
        "            questions.append(question_text)\n",
        "            question_start = interview_text.find(\"START QUESTION\", question_end)\n",
        "        else:\n",
        "            break  # End if no 'END QUESTION' tag is found\n",
        "\n",
        "    return questions\n",
        "\n",
        "# Assuming 'generated_response' contains the output from the model\n",
        "interview_questions = extract_tagged_questions(generated_response)\n",
        "for idx, question in enumerate(interview_questions, 1):\n",
        "    print(f\"Question {idx}: {question}\")\n",
        "    print(infer(question, fine_tuned_model, fine_tuned_tokenizer, True))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHlYZ8OwQ25E",
        "outputId": "56890b81-341a-4f76-e3e1-9320a87e779c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question 1: Johnson, you've had an incredible game tonight. Can you walk us through your mindset when you took that game-winning shot with just seconds left on the clock?\n",
            " Johnson, you've had an incredible game tonight. Can you walk us through your mindset when you took that game-winning shot with just seconds left on the clock?   I think it was a little bit of both but yeah we were able to get out there and win this race because they're going into their own car so I thought our cars would be better than ours today. But at one point in my career if anything maybe not even more good for me now. So hopefully tomorrow is another day where everybody will have some chances again which means really next year we'll see how things go from here until Sunday. We can't wait what happens after Friday's weekend two days' restarts. Hopefully try as hard every restart. It\n",
            "Question 2: This season has been a rollercoaster for the Knights. How do you think this victory will impact the team's morale and performance moving forward?\n",
            " This season has been a rollercoaster for the Knights. How do you think this victory will impact the team's morale and performance moving forward?   I mean it is going to be tough because we have so many guys that are playing really well right now but they're not getting too far ahead of themselves or their players in our clubhouse as much as usual which means there can still get some momentum from them on top off. So hopefully tomorrow we'll see how things go again with those two teams just keep battling each other out here at home field. We've got good pitching. And if anything like today doesn't matter until next year everybody gets healthy then yeah let me say one more game. But no question about us winning every single\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparing to only GPT4 text"
      ],
      "metadata": {
        "id": "wv6fYRXl164C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "def query_openai(prompt, api_key):\n",
        "    url = \"https://api.openai.com/v1/chat/completions\"\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": f\"Bearer {api_key}\"\n",
        "    }\n",
        "    data = {\n",
        "        \"model\": \"gpt-4\",\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "        \"temperature\": 0.1\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
        "    return response.json()\n",
        "\n",
        "# Replace 'your_api_key_here' with your actual OpenAI API key\n",
        "api_key = \"\"\n",
        "\n",
        "prompt = (\"You are a sports caster. Make up a fake play-by-play of a game. \"\n",
        "          \"Describe the actions, the atmosphere, the crowd's reactions. \"\n",
        "          \"Now, transition into an interview with a player. Say 'BEGIN INTERVIEW'. \"\n",
        "          \"Generate two random interview questions, each enclosed with 'START QUESTION' and 'END QUESTION' tags. ALSO INCLUDE THE PLAYER'S ANSWER TO THESE QUESTIONS.\"\n",
        "          \"Conclude the interview with 'END INTERVIEW'.\")\n",
        "\n",
        "\n",
        "response = query_openai(prompt, api_key)\n",
        "generated_response = response['choices'][0]['message']['content']\n",
        "\n",
        "print(generated_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GE77qqTvEZH",
        "outputId": "5bced586-df66-45d0-fee6-7c8946260902"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ladies and gentlemen, we're here at the packed Madison Square Garden for the final game of the season. The atmosphere is electric, the crowd is on their feet, and the tension is palpable. The New York Titans are up against the Los Angeles Legends, and it's a nail-biter.\n",
            "\n",
            "The Titans have the ball. Johnson passes it to Rodriguez, who's been on fire tonight. He dribbles past one, two, three defenders, the crowd is roaring. He's at the three-point line, he shoots... and it's in! The crowd goes wild! The Titans take the lead with just seconds left on the clock. The Legends call for a timeout, but it's clear that the momentum is with the Titans.\n",
            "\n",
            "The buzzer sounds, and it's over! The New York Titans have won the championship! The crowd is ecstatic, the players are hugging each other, tears of joy and relief on their faces. What a game, folks!\n",
            "\n",
            "Now, we're going to transition into an interview with the star of the night, Rodriguez. BEGIN INTERVIEW.\n",
            "\n",
            "START QUESTION: Rodriguez, you've been phenomenal tonight. What was going through your mind when you took that final shot? END QUESTION\n",
            "\n",
            "Rodriguez: Honestly, I was just focused on the basket. I knew I had to make it. I trusted my training, my teammates, and just let it fly. When it went in, it was just pure joy.\n",
            "\n",
            "START QUESTION: This is a huge win for the Titans. How does it feel to be a part of this historic victory? END QUESTION\n",
            "\n",
            "Rodriguez: It's an incredible feeling. We've worked so hard for this, and to see it all come together is just amazing. I'm proud of my team, and I'm grateful to our fans for their unwavering support. This win is for them.\n",
            "\n",
            "END INTERVIEW.\n",
            "\n",
            "There you have it, folks. A night of unforgettable basketball, a historic win for the Titans, and a star performance from Rodriguez. This is why we love this game.\n"
          ]
        }
      ]
    }
  ]
}